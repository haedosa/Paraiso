\documentclass[twocolumn]{article}
\usepackage{amsmath,algorithmic}

% Create link to web,figures (better)
\usepackage[dvipdfmx]{graphicx,hyperref}
\usepackage{hyperref}

% Don't link (failsafe option)
%\usepackage[dvipdfmx]{graphicx}
%\usepackage{url}

%% \setlength{\oddsidemargin}{-0.4mm} 
%% \setlength{\evensidemargin}{\oddsidemargin}
%% \setlength{\textwidth}{170mm} 
\setlength{\textheight}{50\baselineskip}
\addtolength{\textheight}{\topskip}
\setlength{\voffset}{-0.6in}


\bibliographystyle{alpha}

\title{Orthotope Machine}
\author{Takayuki Muranushi}
\begin{document}
\maketitle
\begin{quote}
  In geometry, an {\em orthotope} (also called a hyperrectangle or a box) is
  the generalization of a rectangle for higher dimensions, formally
  defined as the Cartesian product of intervals.

  ``Orthotope'' means ``multidimensional array'' in this document.
\end{quote}

\section{Introduction}

This document describes the {\em Orthotope Machine}, a virtual machine that
operates on multidimensional arrays. The Orthotope Machine is one of the main
components for Paraiso project. The goal of Paraiso project is to create a
high-level programming language for generating massively parallel, explicit
solver algorithms of partial differential equations. 

From astrophysical interest, the Paraiso project will make simulations much
easier for basic equations such as hydrodynamics, magnetohydrodynamics,
general relativity, relativistic radiative transfer and so on. Buiding complex
model from combinations of these basic equations, chemistry, nuclear
reactions, will also become much easier and tractable. The generated program
will support various machines from 1-node GPU workstation to the K computer in
Kobe. One feature that is desired but missing in this document is support for
fixed mesh refinement (FMR). And of course, there are lots of problems in
astrophysics, that do not reduce to solving partial differential equations
explicitly.

From computational viewpoint, explicit solvers of partial differential
equations belongs to the algorithm category called stencil codes. Stencil
codes are algorithms that updates the array, each element accessing the nearby
elements in the same pattern (c.f. Fig.\ref{FigureStencilPseudoCode}). Stencil
codes are commonly used algorithms in fields such as solving partial
differential equations and image processing. Code generations and automated
tuning for stencil codes has been studied e.g. \cite{Datta:EECS-2009-177,
  Datta:2008:SCO:1413370.1413375}.

There are many methods other than stencil codes for solving partial
differential equations. They have different merits. A notable project in
progress is Liszt \cite{Chafi:2010:LVH:1932682.1869527}, an embedded
DSL(domain specific language) in programming language Scala, designed for
generating hydrodynamics solver on unstructured mesh.

\begin{figure}
\begin{verbatim}
double a[NY][NX], b[NY][NX];
for (int t=0; t<max_t; ++t) {
  for (int y=1; y<NY-1; ++y) 
    for (int x=1; x<NX-1; ++x) 
      b[y][x] = a[y][x-1] + a[y][x+1] 
              + a[y-1][x] + a[y+1][x];

  for (int y=1; y<NY-1; ++y) 
    for (int x=1; x<NX-1; ++x) 
      a[y][x] += 0.25 * b[y][x];
}
\end{verbatim}
\caption{An example of stencil code.}\label{FigureStencilPseudoCode}
\end{figure}

Many parallel and distributed programming languages has been implemented using
Haskell \cite{CambridgeJournals:114967}. Data Parallel Haskell
\cite{nested-data-parallelism} and
Nepal\cite{springerlink:10.10073-540-44681-8_76} are implementations of NESL,
a language for operating nested arrays.
Accelerate \cite{Chakravarty:2011:AHA:1926354.1926358} and
Nikola \cite{Mainland:2010:NEC:1863523.1863533} are languages to manipulate arrays on GPUs written in Haskell.

We need new languages for parallel hardwares --- this is a long-standing
idea. Many project sought for them, and some failed. Failures from which we
can learn. High Performance Fortran was a very promising approach to introduce
a high-level parallelism in Fortran but, as James Stone told me in Taiwan, and
as is summarized by the project leader
\cite{Kennedy:2007:RFH:1238844.1238851}, it failed.  DEQSOL
\cite{SAGAWANOBUTOSHI:1989-01-15,Kon'no:1986:AIS:324493.325029} was another
project which had design similar to that of Paraiso. The language was
initially designed for Hitachi vector machines. The extension of DEQSOL for
parallel vector machines has been planned \cite{SagawaNobutoshi:1989-03-15}
but seemingly did not realize.

The unique point of Paraiso compared to those projects is its focus on
computational domains that utilize localized access to multidimensional
arrays. Paraiso is also not a caller of libraries for known algorithms;
rather, it is a tool for testing and implementing new algorithms.

Multidimensional arrays are different from nested arrays. For example in the
pseudocode Fig.\ref{FigureStencilPseudoCode}, in order to calculate {\tt
  b[y][x]} you need to read from {\tt a[y-1][x]} and {\tt a[y+1][x]}, which
are usually located much farther in the memory compared to {\tt a[y][x-32]} or
{\tt a[y][x+64]}. The code generator must be aware of such locality in
multidimensional space.  For most of the cases, the basic equations to be
solved is symmetric under exchange of the axes (X,Y,Z ...). Still, there are
non-negligible differences between the axes from computational point of view,
especially if the multidimensional arrays are stored in row-major or
column-major order. To utilize the cache and/or vector instructions the code
generator need to know and decide upon the order the array is stored in the
memory.

In parallel machines, the array must be decomposed and distributed among
computer nodes. It is important to take care of the continuity in
multidimensional space when making the distribution, so that the
communications cost is lowered. If the data to be communicated is stored
sparsely in the memory, it is a good strategy to gather them manually into a
single buffer and to make a single large communication instead of making lots
of small communications. Due to this gather/scattering cost, not making any
decomposition in one or more directions gives higher performance in some
cases.

The Orthotope Machine is designed to capture and utilize these
characteristics of the multidimensional array computations.

This document is organized as follows.  In \S\ref{SectionPDE}, I describe the
computational natures of the algorithms used to solve the partial differential
equations explicitly. In \S\ref{SectionParaiso}, I describe the overall design
of Paraiso, to clarify the Orthotope Machine's role in it. In
\S\ref{SectionOrthotope}, I give definitions for Orthotope and
Orthotree. \S\ref{SectionHardware} gives an idea for modeling the parallel
computer hardware. \S\ref{SectionAPI} presents a c++ API for
Paraiso. \S\ref{SectionInst} gives the instruction set for Orthotope
Machines. Finally, \S\ref{SectionOptimization} deals with possible
optimization techniques for Orthotope Machines.

In short, I'd like to make something called Paraiso, that takes DPDEL
(Fig.~\ref{FigureDPDEL2}) as the input, then translates it to Orthotope
Machine instruction (Fig.\ref{FigureOMInst}), then to native program like
(Fig.\ref{FigureHydroPseudoCode}), and generates a solver library with API
described in \S\ref{SectionAPI}.


\section{Explicit Solvers of Partial Differential Equations}
\label{SectionPDE}


To begin with, let me describe what kind of problem I want to solve.

\begin{figure}
  \vspace{-5cm}
  \includegraphics[scale=0.4]{figure/fluid.eps}
  \caption {Schematic image of how a fluid simulator
    works. }\label{FigureFluidScheme}
\end{figure}



\begin{figure*}
\begin{verbatim}
double fluid[NZ][NY][NX];
double flow_x[NZ][NY][NX];
double flow_y[NZ][NY][NX];
double flow_z[NZ][NY][NX];
double dt_local[NZ][NY][NX];

// (1) simulation goes from time t=0 to t=t_max
for (double t=0; t<t_max; t+=dt) {

  // (2) calculate the timescale for each mesh
  for (int z=1; y<NZ-1; ++z) 
    for (int y=1; y<NY-1; ++y) 
      for (int x=1; x<NX-1; ++x) 
        dt_local[z][y][x]=timescale(fluid[z][y][x]);

  // (3) calculate the minimum timescale
  double dt=max_t;
  for (int z=1; y<NZ-1; ++z) 
    for (int y=1; y<NY-1; ++y) 
      for (int x=1; x<NX-1; ++x) 
        dt=min(dt, dt_local[z][y][x]);

  // (4) calculate the flow for each direction
  for (int z=1; y<NZ; ++z) {
    for (int y=1; y<NY; ++y) { 
      for (int x=1; x<NX; ++x) { 
        flow_x[z][y][x]=calc_fx(fluid[z][y][x-1], fluid[z][y][x]);
        flow_y[z][y][x]=calc_fy(fluid[z][y-1][x], fluid[z][y][x]);
        flow_z[z][y][x]=calc_fz(fluid[z-1][y][x], fluid[z][y][x]);
      }
    }
  }

  // (5) move the fluid according to the flow
  for (int z=1; y<NZ-1; ++z) 
    for (int y=1; y<NY-1; ++y) 
      for (int x=1; x<NX-1; ++x)
          fluid[z][y][x] += dt*(
            (flow_x[z][y][x]-flow_x[z][y][x+1])/dx +
            (flow_y[z][y][x]-flow_x[z][y+1][x])/dy +
            (flow_z[z][y][x]-flow_x[z+1][y][x])/dz);

}
\end{verbatim}
  \caption{A pseudocode showing the typical design of a hydrodynamic
    equations solving algorithm.  c.f.  Fig. \ref{FigureFluidScheme}
  }\label{FigureHydroPseudoCode}
\end{figure*}

Let us take for example a hydrodynamics solver that represent the fluid by a
mesh structure c.f. Fig. \ref{FigureFluidScheme}. Each mesh has volume
$\mathit{dx} \mathit{dy}$. Each mesh stores the amount of fluid wihin that
volume. The solver's algorithm is to calculate the flows of the fluid across the
mesh boundaries, and add/subtract the amount of fluid.

Fig. \ref{FigureHydroPseudoCode} shows a pseudocode for a
three-dimensional hydrodynamic equations solver. Although it is a bit
simplified to be a real solvers, it shows what components needed to
build one.

A common task for the solver is to simulate the evolution of the fluid for a
certain interval of time, say, $0 < t < t_{\mathrm max}$ (1). In the each step
of the simulation, the time {\tt t} increases by a certain amount {\tt dt}.
In many problems, the time-step {\tt dt} is not a constant but depends on the
state of the fluid. In such case, you need to calculate the time-steps adequate
for the fluid state of every mesh (2), and then perform reduction over the
entire computational domain to calculate the smallest {\tt dt} (3). Then,
amount of fluid flow across X, Y, and Z mesh boundaries are calculated from
the fluid state of the meshes that are adjacent to the boundaries (4). 

To generate a real-world fluid solver, there are more complexity compared to
the example in Fig. \ref{FigureHydroPseudoCode}. First, the {\tt fluid} has
not one but several component. For example, we typically need five 3D arrays
(or an array of a struct with five elements) to solve hydrodynamics. They are
density, three components of the velocity, and energy. For
magneto-hydrodynamics, we need eight components. For general relativity we
need twenty-one.

Second, the size of the stencil (the number of input mesh needed to calculate
the answer for one mesh) is larger. {\tt calc\_fx} in
Fig. \ref{FigureHydroPseudoCode} reads two mesh. In real solvers, they may
read four, six or more meshes, to achieve higher-order interpolation in space.
Third, the entire solver will be replicated several times, to achieve
higher-order time integral. Runge-Kutta method is a well known example of such
a technique. This also make the stencil larger.

Too large stencil is a bad thing. With larger stencil we need to perform more
duplicated calculations, and we need communication buffer so large that
distributed computation do not contribute to better performance. So we need to
split the algorithm into several components with smaller stencil. Splitting it
into too many pieces, is again not a good idea. We must search for the middle
path.

To get a feeling of a real ``real-world solver,'' please look at {\tt
  proceed} function of my magneto-hydrodynamics solver I'm working
right now :
\url{https://github.com/nushio3/nmhd/blob/master/src/library/mhd_solver.inl}
. I just wanted you to see how dirty the code is and how it repeats
itself. The code is actually generated from a more compact, but less
readable ruby script :
\url{https://github.com/nushio3/nmhd/blob/master/src/library/mhd_solver.inlrb}
.



\section{Overall Design of Paraiso}
\label{SectionParaiso}

\begin{table*}
  \begin{tabular}{|l|l|l|}
    \hline 
    Language                             &  Sample code & Handled by \\
    \hline 
    DPDEL                                & Fig.\ref{FigureDPDEL1},\ref{FigureDPDEL2} & DPDEL monad, quasiquoter and parser     \\
    Primordial Orthotope Machine AST     & Fig.\ref{FigureOMInst} & Orthotope Machine divider \\
    Distributed Orthotope Machine AST    & & Orthotope AST optimizer and compiler \\
    Native codes (Fortran,C,CUDA,OpenCL with MPI) & Fig.\ref{FigureHydroPseudoCode} & Native compilers\\
    Executables                          & & Real machines \\
    \hline
  \end{tabular}
  \caption{
    The software stack of Paraiso, through which DPDEL programs are translated to executables.
  }\label{TableSoftwareStack}
\end{table*}

In this section I briefly describe the overall design of Paraiso, to clarify
the Orthotope Machine's role in it. Please also
c.f. \url{http://paraiso-lang.org/wiki/index.php/Grand_Design}.

Paraiso translates the input programs into native programs with several steps:
c.f. Table \ref{TableSoftwareStack}. The input language is DPDEL (Discretized
Partial Differential Equation Language); the language to describe the
numerical simulation algorithms, in as simple form as possible
c.f. \url{http://paraiso-lang.org/wiki/index.php/DPDEL}.  Then it is
translated to Orthotope Machine (OM) program. Its name used to be Virtual
Vector Machine (VVM) in original Paraiso proposal c.f.
\url{http://paraiso-lang.org/wiki/index.php/VVM}. There are two versions of
Orthotope Machine: one is Primordial OM, the other is Distributed OM.  DPDEL
program is first translated to Primordial OM program, on which it can use
arbitrary large arrays. Then ``machine division'' operations are applied,
according to the hardware specifications, producing Distributed OM
instructions. Distributed OM instructions are then translated to native
codes. Native codes are compiled by existing compilers, and finally, all
programs are executable.



\subsection{Discretized Partial Differential Equation Language (DPDEL)}

DPDEL program is constructed within a monad: This technique is
described in tanakh's blog \url{http://d.hatena.ne.jp/tanakh/20070918}
and is featured in the first prototype of Paraiso. The monad is called
{\tt Dsl} and defined in
\url{https://github.com/nushio3/Paraiso/blob/master/attic/paraiso-2008-ODEsolver/Paraiso.hs}.


Fig.\ref{FigureDPDEL1} shows the possible DPDEL program. Although it is fairly
readable, it has some shabby points.
\begin{itemize}
  \item Calculations for X,Y,Z directions are repeated,
  \item Vectors are represented in scary ways: {\tt ((-1):.0:.0)},
  \item Most part of the program is to assign an expression {\tt expr} to a
    new AST node {\tt a}, but you cannot write {\tt a <- expr} since {\tt a}
    is also an expression and {\tt a} and {\tt expr} have the same type; {\tt
      expr} cannot be a monad that returns {\tt expr}.  Expression must go
    through some function e.g. {\tt bind :: AST a -> DPDEL (AST a)}.
  \item You are forced to use customized operators {\tt+. *.} ... instead of
    usual operators {\tt+ *}... . This may not happen in the arithmetic
    operators, but at least it surely happens for comparison operators such as
    {\tt>}, because their type is {\tt a -> a -> Bool}. You cannot get it have
    the type {\tt AST a -> AST a -> AST Bool}. This barrier is also observed
    in Nikola, a DSL for GPU in Haskell
    \cite{Mainland:2010:NEC:1863523.1863533}. Well, you can, in Haskell, but
    in nasty ways.
\end{itemize}

Fig.\ref{FigureDPDEL2} shows another possible version of DPDEL with
above drawbacks removed.  First improvement is the concept of Axis. In
the sample code, axis has two role.  One is to specify a certain
component of three-dimensional vector, e.g. {\tt flow\_axis}. The
other is to shift the indices for three-dimensional arrays. The
indices are represented as lambda-bound variable {\tt o} and shifted
like {\tt [o+axis]}.

This double-roled Axis is invented for my magnetohydrodynamics
program. The two role combined, it really helps writing template
functions, instantiated three times, each of which handles one of
X,Y,Z directions. The concept of Axis is defined in
\url{https://github.com/nushio3/nmhd/blob/master/src/library/direction.h}
as a set of dummy structs {\tt XAxis, YAxis, Zaxis} etc. It is
extensively used to construct three-dimensionally accessible arrays
\url{https://github.com/nushio3/nmhd/blob/master/src/library/crystal.h}
as well as to write the main program.

But how does Haskell know that the name {\tt flow\_axis} is related with the
name {\tt axis}? Isn't {\tt flow\_axis} a single name identifier in Haskell
syntax? Here comes the second point.

Let me introduce {\bf \tt qb}, quasiquote-and-bind operation. This lifts the
constraints coming from the Haskell syntax. It parses its argument string and
does several things: it changes {\tt flow\_axis} into something like {\tt
  getComponent axis flow}, and translates {\tt [o+axis]} into type-correct
expression like {\tt [o+getUnitVector axis]}, thus enables the use of
double-roled Axis with succinct notation. It renames other arithmetic
operators appropriately.  It has implicit antiquotes. Finally, {\tt qb} adds
{\tt bind} to the entire expression. {\tt qb} grants us magical power.

Well, although what {\tt qb} offers us seem so alluring, things may not go too
well with {\tt qb}. We may find some of the facilities described above
impossible to implement. We may need several quasiquoters instead of just
one. Even so, use of quasiquote is essential in making DPDEL syntax simpler.

\begin{figure*}
\begin{verbatim}
timescale = ...
calcFX f1 f2 = ...
calcFY f1 f2 = ...
calcFZ f1 f2 = ...

proceed :: DPDEL ...
proceed = do
  t <- input $ Orthotope0
  fluid <- input $ Orthotope3
  dtLocal <- bind $ timescale fluid
  dt <- bind $ reduce Min dtLocal

  flowX <- bind $ calcFX fluid (shift ((-1):.0:.0) fluid)
  flowY <- bind $ calcFY fluid (shift (0:.(-1):.0) fluid)
  flowZ <- bind $ calcFZ fluid (shift (0:.0:.(-1)) fluid)
  
  newFluid <- bind $ fluid +. dt *. (
    (flowX -. shift (1:.0:.0) flowX)/.dx +
    (flowY -. shift (0:.1:.0) flowY)/.dy +
    (flowZ -. shift (0:.0:.1) flowZ)/.dz )

  output (t+dt) t
  output newFluid fluid

\end{verbatim}
\caption{A DPDEL monad that corresponds to the pseudocode in
  Fig. \ref{FigureHydroPseudoCode}.}\label{FigureDPDEL1}
\end{figure*}

\begin{figure*}
\begin{verbatim}
timescale = ...
calcF axis f1 f2 = ...

proceed :: DPDEL ...
proceed = do
  t <- input $ Orthotope0
  fluid <- input $ Orthotope3
  dtLocal <- bind $ timescale fluid
  dt <- bind $ reduce Min dtLocal

  flow <- forAxes (\axis ->
    [qb| o -> calcF axis fluid[o] fluid[o-axis]] )

  
  updates <- forAxes (\axis -> [qb| o-> (flow_axis[o] - flow_axis[o+axis]) / dr_axis ]

  newFluid = [qb| o-> fluid[o] + dt * (sum updates)[o]]

  output (t+dt) t
  output newFluid fluid

\end{verbatim}
\caption{A more sophisticated DPDEL code using quasi-quote and Axis. {\bf qb}
  is a quoted bind. The argument of {\bf qb} is parsed by a separate parser
  that understands e.g. the Einstein rules, and a {\bf bind} is added.
  Compare with Fig. \ref{FigureDPDEL1}.}\label{FigureDPDEL2}
\end{figure*}

\subsection{Orthotope Machine}

DPDEL programs are translated into programs for the Orthotope Machine,
the main topic of this document. The OM program describes data-flow
from one orthotope to others, in static single assignment (SSA) style
machine-language-flavor program, forming a directed acyclic graph. The
DPDEL program in Fig. \ref{FigureDPDEL2} will translate to something
like Fig. \ref{FigureOMInst} . Also look at
\url{https://github.com/nushio3/Paraiso/blob/master/attic/paraiso-2010-lifegame/}
for the working example, the second prototype of Paraiso. When you
{\tt make} this program, you'll see the Conway's game of life
running. {\tt Main.hs} contains the program for the cellular
automata. {\tt VVM.hs} defines the instruction set and the syntax
tree, and {\tt VVMCompiler.hs} is a compiler.

\begin{figure*}
\begin{verbatim}
t <- input 
fluid <- input
dtLocal <- ...
dt <- reduce dtLocal

a1 <- shift ((-1):.0:.0) fluid
flowX   <- arith calcFX fluid a1
a2 <- shift (0:.(-1):.0) fluid
flowY   <- arith calcFY fluid a2
a3 <- shift (0:.0:.(-1)) fluid
flowZ   <- arith calcFZ fluid a3

a4 <- shift (1:.0:.0) flowX
a5 <- arith (-) flowX a4
a6 <- arith (/) a5 dx
a7 <- shift (0:.1:.0) flowY
a8 <- arith (-) flowY a7
a9 <- arith (/) a8 dy
a10 <- shift (0:.0:.1) flowZ
a11 <- arith (-) flowZ a10
a12 <- arith (/) a11 dz
a13 <- arith (+) a6 a9
a14 <- arith (+) a13 a12
a15 <- arith (*) a14 dt
a16 <- arith (+) a15 fluid

newT <- arith (+) t dt

output newT t
output a16 fluid
\end{verbatim}
  \caption{A sketch of how an Orthotope Machine instruction will look
    like.}\label{FigureOMInst}
\end{figure*}

\newpage
~
\newpage
~
\clearpage


\section{Orthotope and Orthotree}
\label{SectionOrthotope}
\subsection{Orthotope}

Orthotope is a rectangle in $n$-dimension. A zero-dimensional orthotope is a
point; an one-dimensional orthotope is an interval; a two-dimensional
orthotope is a rectangle and a three dimensional orthotope is a box. The
intersection of two orthotopes is also an orthotope, but union of two
orthotopes is not. The set of all $n$-dimensional orthotopes is closed under
intersections, but not under unions. Such system is called a $\pi$-system in
mathematics.

\begin{verbatim}
class PiSystem a where
  empty :: a
  null :: a -> Bool
  intersection :: a -> a -> a
\end{verbatim}

We do not introduce union between two orthotopes. The reason why we don't need
union will be explained in the next subsection.

An interval is either empty, or a pair of two ordered elements. Interval is an
instance of $\pi$-system.

\begin{verbatim}
data Interval a = Empty |
     Interval {lower::a, upper::a}
instance (Ord a) => PiSystem (Interval a) 
  where
\end{verbatim}

A zero-dimensional orthotope can have two unit state: it can either be vacant
({\tt Z0}) or occupied ({\tt Z}). Orthotopes of higher dimensions are built
using the {\em snoc} operator {\tt (:.)} borrowed from Repa
\cite{Keller:2010:RSP:1932681.1863582}.

\begin{verbatim}
data Orthotope0 a = Z0 | Z

type Orthotope1 a=Orthotope0 a:.Interval a
type Orthotope2 a=Orthotope1 a:.Interval a
type Orthotope3 a=Orthotope2 a:.Interval a
\end{verbatim}

For more detail, please refer to the source
\url{https://github.com/nushio3/Paraiso/blob/master/Language/Paraiso/Orthotope.hs}
and example
\url{https://github.com/nushio3/Paraiso/blob/master/attic/TestOrthotope.hs} .

\subsection{Orthotree} \label{SectionOrthotree}

\begin{figure}
  \includegraphics[scale=0.5]{figure/orthotree.eps}
  \caption{An Orthotree with five leaves.}
  \label{FigureOrthotree}
\end{figure}

So far, we assumed a single ideal machine with infinite amount of memory. Real
machines have finite memories and consist of distributed components connected
with limited bandwidth.  So we have to fit the ideal into the reality.

We first start from an infinitely large orthotope, and cut it recursively
until each computer node has a portion of the orthotope. Introducing boundary
conditions and domain splitting are treated uniformly. Either of the
operations introduces restrictions on the parent orthotope, and generates
needs for additional communications so that the divided calculations is
identical to the calculations before division.

The recursive division of an Orthotope is denoted as an Orthotree :
\begin{verbatim}
data Orthotree a = 
  Leaf |
  Warp Axis (Interval a) Orthotree |
  Divide Axis a Orthotree Orthotree
\end{verbatim}

Here, {\tt Warp} introduces a cyclic boundary condition, and {\tt Divide}
denotes an orthotope division. For example,

\begin{verbatim}
t :: Orthotree Int
t = Warp Y (Interval 0 60) $
    Warp X (Interval 0 60) $
    Divide Y 24 
      (Divide X 30 Leaf Leaf)
      (Divide X 20 Leaf
        (Divide X 40 Leaf Leaf))
\end{verbatim}

denotes the structure in Fig.~\ref{FigureOrthotree}. Orthotree is a variation
of e.g. oct-trees, the data structures invented to represent domain
decompositions.

The reason why we do not introduce union between orthotopes is that we can
always back-trace this Orthotree. Unions are allowed only between orthotopes
that are apart but used to be united. Those are only kinds of unions that we
need, and the union results are always guaranteed to be orthotopes.

\subsection{Load Balancing}

Generally speaking, boundary indices are not compile-time constants. This is
because each MPI instance of the program do not know which portion of the
Orthotree to solve until execution. Moreover, we can design Distributed OM so
that boundary indices are changeable at runtime. It opens path to dynamic load
balancing. If we achieve that, and also achieve the overlapped communication
and calculation, any program that is generated from Paraiso and communication
time is less than computation time will achieve near 100\% machine utilization
on any heterogeneous architecture. This is quite amazing.


\section{Hardware Model}
\label{SectionHardware}

\begin{figure*}
  \begin{center}
    \includegraphics[scale=0.3]{figure/hardware_graph.eps}
  \end{center}
  \caption{Possible hardware model.}\label{FigureHardware}
\end{figure*}

Machine division is performed to fit a target parallel computer, so we
need to describe the hardware configuration in machine-readable form.
Fig.~\ref{FigureHardware} shows a hardware model, and is generated by
graphViz program --- from machine readable {\tt .dot} graph
definition. So there's some hope. 

In Fig.~\ref{FigureHardware} some ideas for the hardware model is
presented. There are three kinds of node in the graph : processors, storages,
and interconnects. Processors and storages can not be directly connected. You
must interleave at least one interconnect node.

Each hardware node is labeled in the triplet {\tt method : tag :
  performance}. The {\tt method} represents the knowledge upon how to use the
hardware: for processors, how to perform calculations and access their primary
storage; for storages, how to allocate the required amount of storage spaces;
for interconnects, how to move data from one side to the other. The {\tt
  method} field is actually a code generator that takes part of a Distributed
OM instructions and generates the appropriate codes for the request. We still
have to make surveys and experiments on how we interface these code
generators.

The {\tt performance} field shows some quantitative performance of the
hardware: for processors, its peak performance; for storage, its capacity; for
interconnects, its bandwidth. More performance can be added, such as latency,
power consumption, sustained performance of some sort, etc. The {\tt
  performance} field is used to determine the initial machine division, which
will then slightly modified via benchmarking and dynamic load-balancing.

Subgraphs with multiplier label e.g. {\tt 144x} are used to represent the
identical component in the model.



\section{API for Orthotope Machine}
\label{SectionAPI}

An orthotope machine definition consists of
\begin{itemize}
  \item a list of name and shape of Static Orthotopes, and
  \item one or more {\em kernel} definitions in DPDEL. Kernels are programs
    that modifies the Static Orthotopes.
\end{itemize}

There are Static and Local Orthotopes. Static Orthotopes, like static
variables, survive kernel calls. On the other hand, Local Orthotopes exist
within OM AST. There are lots of them, but most of them don't even get
allocated, due to loop fusions and other optimizations.

Kernels are large-grained portion of solver program. We do not attempt right
now to write the entire solver in DPDEL; instead, we prefer to write a few
kernels and control them from more sophisticated languages. Simple solver may
consist of two kernels, one for setting the initial condition of the
simulation, and the other is for performing the one step of the simulation.

An API for Distributed Orthotope Machine has
\begin{itemize}
  \item an interface to read/write any 0-dimensional static orthotopes
  \item interface for invoking each kernels
  \item a function that causes the entire machine state saved to a file
  \item a function that loads from such save-points
\end{itemize}

This will do for simple simulation tasks. A c++/MPI interface will be a good
starting point. Upon constructing the solver, all but rank0 goes to a passive
mode, and APIs are called from the rank0 node.

\begin{verbatim}
Solver s;
s.load("old");
if (mpi_rank!=0) {
  s.passive();
} else {
  s.set_initial_condition();
  for(s.t()=0; s.t() < time_max;
      s.t()+=s.dt()){
    s.proceed();
  }
}
s.save("new");
\end{verbatim}

We can easily imagine interfaces for other languages.


\section{Instruction Set}
\label{SectionInst}

Orthotope Machine (OM) is a virtual machine that has an instruction set
operating on orthotopes. Each instruction is presented in static single
assignment (SSA) form. For example, {\tt add} instruction in common CPUs or in
LLVM adds two scalar value. The {\tt add} instruction in Orthotope Machine
takes two orthotope and returns one, acting much like {\tt zipWith (+)}.

\subsection{Static Addressed Single Assignment} \label{SectionSASA}

We extend the static single assignment (SSA) form concept to static addressed
single assignment(SASA) form.  Consider the following DPDEL program, where
{\tt a,b,c} are orthotopes:
\begin{verbatim}
b <- shift (-1) a * shift 1 a;
c <- shift (-1) b + b + shift (+1) b;
\end{verbatim}
the program is in SSA (static single assignment) form.
If we generate the following code from the DPDEL, it will give a wrong result.
\begin{verbatim}
double a[N], b[N], c[N];
for (int i=2; i<N-2; ++i) {
  b[i] =a[i-1]*a[i+1];
  c[i] =b[i-1]+b[i]+b[i+1];
}
\end{verbatim}
In parallel language such as CUDA, the execution order of the loop is
nondeterministic, so the result of the above program is not even
well-defined. In SSA (static single assignment) form, every variable is only
assigned once. However, there remains some ambiguity.  Assigning {\em each
  orthotope} variable only once does not fix the meaning of program uniquely
in such nondeterministically parallel environment.

One way to fix this is to assure that {\em each element of each orthotope} is
statically assigned at most once. To do this, first we must detect how the
output ({\tt c[i]} in this case) depends on every SSA orthotope and how large
is the stencil (dependency size). Then we can generate code like this:
\begin{verbatim}
double a[N], c[N];
for (int i=2; i<N-2; ++i) {
  a_i_minus_2 = a[i-2];
  a_i_minus_1 = a[i-1];
  a_i = a[i];
  a_i_plus_1 = a[i+1];
  a_i_plus_2 = a[i+2];
  b_i_minus_1 = a_i_minus_2 + a_i;
  b_i = a_i_minus_1 + a_i_plus_1;
  b_i_plus_1 = a_i + a_i_plus_2;
  c_i = b_i_minus_1 + b_i + b_i_plus_1;
  c[i] = c_i;
}
\end{verbatim}
Let us call this SASA (Static and Addressed Single Assignment)
form. Generating programs in SASA form introduces a lot of redundant
calculations. But it produces correct results.

At any point of execution timeline, we can insert a global
synchronization, thus dividing the abstract syntax tree into several
parts, like this:

\begin{verbatim}
b <- shift (-1) a * shift 1 a;
-- cut here --
c <- shift (-1) b + b + shift (+1) b;
\end{verbatim}

From this AST, following code is generated:

\begin{verbatim}
double a[N], b[N], c[N];
for (int i=1; i<N-1; ++i) {
  a_i_minus_1 = a[i-1];
  a_i_plus_1 = a[i+1];
  b[i] = a_i_minus_1 + a_i_plus_1;
}
for (int i=2; i<N-2; ++i) {
  b_i_minus_1 = b[i-1];
  b_i = b[i];
  b_i_plus_1 = b[i+1];
  c_i = b_i_minus_1 + b_i + b_i_plus_1;
  c[i] = c_i;
}
\end{verbatim}

Synchronization insertion reduces the number of arithmetic operations
performed per mesh, at the cost of increased number of load/store
operations and increased storage consumption. Since the ratio of
calculation speed to storage access speed vary from hardware to
hardware, we need to search for the best number and place to insert
synchronizations for each hardware to achieve maximum performance .




\subsection{Instruction Set for Primordial Orthotope Machine}
Primordial OM is an imaginary machine with infinite amount of memory and
infinitely long vector arithmetic instructions. 

To begin with, let us assume that each OM has a fixed dimension $n$. An
$n$-dimensional OM only deals with $n$-dimensional orthotopes and
0-dimensional orthotopes. 0-dimensional orthotopes are just variables.

See also \url{http://paraiso-lang.org/wiki/index.php/VVM} for VVM instruction
set plan, an ancestor of OM.

For this and next section, let us denote {\tt n}-dimensional Orthotope of
element type {\tt a} as {\tt O n a}.

\paragraph{reduce and broadcast}
\begin{verbatim}
reduce :: ReduceOp -> O n a-> (O 0 a)
broadcast :: (O 0 a)-> O n a
\end{verbatim}
{\tt reduce} creates a 0-dimensional orthotope from an n-dimensional orthotope, by {\tt
  ReduceOp}, an associative operator. This will possibly use MPI\_Reduce, and
thus {\tt ReduceOp} will be restricted to one of {\tt Max}, {\tt Min}, {\tt
  Sum}, {\tt Multiply}.

{\tt broadcast} creates an n-dimensional orthotope from a 0-dimensional
orthotope, simply copying its value.

\paragraph{shift}
\begin{verbatim}
shift :: (Vector n) -> O n a-> O n a
\end{verbatim}
Coordinate-shift the ingredient of the orthotope.  The shift vector must be a
small, and compile-time constant value.

For example, 
\begin{verbatim}
b <- shift (sx:.sy:.sz) a
\end{verbatim}
means
\begin{verbatim}
for (int z=0; y<NZ; ++z) 
  for (int y=0; y<NY; ++y) 
    for (int x=0; x<NX; ++x) 
      b[z][y][x] = a[z-sz][y-sy][x-sx];
\end{verbatim}

\paragraph{arithmetic operations}
\begin{verbatim}
imm :: a -> O n a
add :: O n a -> O n a -> O n a
cmp :: O n a -> O n a -> (O n Bool)
hatena :: (O n Bool) -> O n a -> O n a
                     -> O n a
sincos :: O n a -> (O n a, O n a)
\end{verbatim}
There are arithmetic operations of various kind. Arithmetic operations can
generally have $n_i$ input and $n_o$ output, but all the reads and writes go
to the same coordinate in an orthotope. 

Possibly, we can summarize the various arithmetic operations to a single
polymorphic instruction {\tt arith}. We can think of a way to store the
information on generating arithmetic operations for various hardware in
pluggable manner.

The shapes of the output orthotopes of an {\tt arith} are the intersection
of all the input orthotopes.


\subsection{Instruction Set for Distributed Orthotope Machine}
Distributed Orthotope Machine operates on an Orthotree. Each leaf of the
Orthotree is tied to a Storage in hardware representation. Orthotope now has
the form {\tt (O (Maybe Storage) n a)}, where {\tt Maybe Storage} denotes the
storage the orthotope is stored. If it's {\tt Nothing}, then it's a global value
(possibly stored at MPI rank 0.)

\paragraph{communication}
\begin{verbatim}
communicate :: Storage -> Storage -> O s n a
            -> O s n a
\end{verbatim}
Move data from one storage to other.

\paragraph{split and merge}
\begin{verbatim}
split :: Cutter -> O s n a
      -> (O s n a, O s n a)
merge :: Cutter -> O s n a -> O s n a
      -> O s n a
\end{verbatim}
{\tt split} divides an orthotope into two, and {\tt merge} does the opposite.
As is mentioned in \S\ref{SectionOrthotree}, {\tt merge} instruction is not
issued arbitrary, but only for those orthotopes that was once united. As long
as this rule is obeyed, there's no worry of an impossible merge request.

\section{Possible Program Transformations}
\label{SectionOptimization}
\subsection{Common Techniques}
A lot of important optimization techniques are known for scalar
processors. Some of them are equally applicable to Orthotope Machine
AST.  Even if we emit the code without such optimizations, we can
expect the native compilers do the job for us. At least Paraiso should
not hinder native optimizations. It's better if we can optimize OM AST
using modern optimization platform such as LLVM.

\paragraph{Constant Folding}
Constant folding is to calculate constant expression in compile
time. It'll be fairly easy, and if we forget to do it, native
compilers are also good at it. One very stupid thing is to store a
globally constant value onto an array. We should avoid this.

\paragraph{Loop Fusion}

Loop fusion is a source of high performance in Haskell's array operating
libraries {\tt Data.Vector} and {\tt Data.Array.Repa}
\cite{Keller:2010:RSP:1932681.1863582}, and also in many other libraries in
other languages.


\paragraph{Common Subexpression Elimination}

To avoid performing the same calculation twice, by storing the intermediate
value to the memory. 

\subsection{Time-step Fusion}

Fuse several time-step into one kernel, and access to the storage only at the
beginning and end of the fused kernel. This increases the ratio of
calculation per storage access. This is easier if the time-step is always a
constant and we do not need a global reduce within a kernel. The simulation
for relativistic systems meets such criteria because time-step is always bound
by the speed of light.

\subsection{Use of Scratchpad}

Some processors have scratchpad regions, where programmers manually store some
data and have fast access. GPU's scratchpad has size of tens of kilobytes. On
the other hand, when we attempt a ultra-high-resolution simulation where we
store the simulation state on larger but slower storage such as SSDs, the main
memory itself can be regarded as large scratchpad. 


\subsection{AST Manipulations by User Specification}
One of the reason High Performance Fortran (HPF) has failed
\cite{Kennedy:2007:RFH:1238844.1238851} is because the user of HPF did not
have fine-grained control over the generated code.  First of all, the machine
generated code like Fig.~\ref{FigureOMInst} is unreadable.

I'm thinking of introducing an annotation operator like Parsec's {\tt <?>}
(c.f. \url{http://bit.ly/ef15ty}) to Paraiso. You can choose the name for the
AST nodes so that the code is more tractable. One good idea is that every
identifier that passes {\tt qb} gets annotated as well as antiquoted.

AST is a large, directed acyclic graph and we have large degree of freedom in
how we fold it into one time-line. Here, users can also add pragmas such as
{\tt (<?> balloon)} and {\tt (<?> stone)} to the DPDEL code portion, so that the
portion get executed as soon/late as possible.

\subsection{Synchronization Insertion}

This issue is mentioned in \S\ref{SectionSASA}.  At any point of execution
time-line, we can insert a global synchronization, thus dividing the abstract
syntax tree into several parts. This reduces the number of arithmetic
operations performed per mesh, at the cost of increased number of load/store
operations and increased storage consumption.

The paragraphs of the programs, such as steps of the higher order Runge-Kutta
method, are good candidate for synchronization insertion. Often the actual
programs are written so. So before implementing an automated synchronization
insertion, we can let users provide synchronization point candidates, and try
and benchmark on them.

\subsection{Cursored Arrays}


\subsection{Trapezium Splitting}

Trapezium splitting (c.f. Fig.~\ref{FigureTrapezium}) is a transformation of
one computation to a set of computation that requires less storage space. To
perform the trapezium splitting, first split the output orthotope into several
smaller portions. Then for each of the small orthotope, load the portion of
the input such that the output portion depends on, and calculate the answer
for the portion. Repeat this process until you obtain results for all the
output.

This method is fairly easy, and can be a building block for other optimization
such as use of scratchpad and computation / communication
overlapping. However, trapezium splitting introduces duplicated computations.


\begin{figure*}
  \includegraphics[scale=0.5,angle=270]{figure/trapezium.eps}
  \caption{Trapezium Splitting}\label{FigureTrapezium}
\end{figure*}

\subsection{Parallelogram Splitting}

Parallelogram splitting (c.f. Fig.~\ref{FigureParallelogram}) is an improved
version of trapezium splitting. The difference is that in parallelogram
splitting, after every computation for a portion you keep the ``rightmost
states'' within the scratchpad so that duplicated computations do not happen.

\begin{figure*}
  \includegraphics[scale=0.5,angle=270]{figure/parallelogram.eps}
  \caption{Parallelogram Splitting}\label{FigureParallelogram}
\end{figure*}

\subsection{Block Skew}
Fluid consists of multiple orthotope of the same shape and extent. As in
Fig.~\ref{FigureFluidScheme}, some components are half-mesh shifted. If we
shift some fluid components by half or one mesh, it may alter the performance.



\subsection{Batch Reduce/Broadcast}
Reduce and broadcast needs global communications. To have large-grained
communications, it will be a good idea to perform many reduce/broadcast
instructions at once. 

\subsection{Overlapped Computation / Communication}

It's good if we can detect the portions of calculations that do not depend on
the broadcast data, and perform such calculations in parallel while the
communications or reduces/broadcasts are going on. We may need trapezium or
parallelogram splitting to detach some of the computation regions from the
rest that are waiting for incoming data.

More generally speaking, the Orthotope Machine programs represented in
the form of data flow graphs naturally contain paralellism. The
branches that do not depend on each other can be executed in
parallel. This may consist of computation-communication pair, a pair
of communication that uses different processors or different parts of
a same processor.

\bibliography{paraiso}
\end{document}

